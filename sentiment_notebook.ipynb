{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Step: 0\n",
      "Summary step: 0\n",
      "Train loss: 0.744185\n",
      "Train accuracy:45.703%\n",
      "Test loss:3.210\n",
      "Test accuracy:50.080%\n",
      "Epoch: 0 Step: 50\n",
      "Summary step: 50\n",
      "Train loss: 0.528763\n",
      "Train accuracy:72.656%\n",
      "Test loss:0.516\n",
      "Test accuracy:74.102%\n",
      "Epoch: 0 Step: 100\n",
      "Summary step: 100\n",
      "Train loss: 0.512814\n",
      "Train accuracy:75.781%\n",
      "Test loss:0.510\n",
      "Test accuracy:74.705%\n",
      "Epoch: 0 Step: 150\n",
      "Summary step: 150\n",
      "Train loss: 0.494052\n",
      "Train accuracy:75.781%\n",
      "Test loss:0.478\n",
      "Test accuracy:76.576%\n",
      "Epoch: 0 Step: 200\n",
      "Summary step: 200\n",
      "Train loss: 0.459193\n",
      "Train accuracy:77.148%\n",
      "Test loss:0.481\n",
      "Test accuracy:76.618%\n",
      "Epoch: 1 Step: 0\n",
      "Summary step: 250\n",
      "Train loss: 0.482597\n",
      "Train accuracy:77.148%\n",
      "Test loss:0.466\n",
      "Test accuracy:77.476%\n",
      "Epoch: 1 Step: 50\n",
      "Summary step: 300\n",
      "Train loss: 0.456263\n",
      "Train accuracy:77.930%\n",
      "Test loss:0.502\n",
      "Test accuracy:75.798%\n",
      "Epoch: 1 Step: 100\n",
      "Summary step: 350\n",
      "Train loss: 0.478091\n",
      "Train accuracy:77.734%\n",
      "Test loss:0.458\n",
      "Test accuracy:78.010%\n",
      "Epoch: 1 Step: 150\n",
      "Summary step: 400\n",
      "Train loss: 0.449022\n",
      "Train accuracy:78.711%\n",
      "Test loss:0.451\n",
      "Test accuracy:78.180%\n",
      "Epoch: 1 Step: 200\n",
      "Summary step: 450\n",
      "Train loss: 0.438439\n",
      "Train accuracy:79.297%\n",
      "Test loss:0.456\n",
      "Test accuracy:78.619%\n",
      "Epoch: 2 Step: 0\n",
      "Summary step: 500\n",
      "Train loss: 0.457904\n",
      "Train accuracy:78.125%\n",
      "Test loss:0.451\n",
      "Test accuracy:78.519%\n",
      "Epoch: 2 Step: 50\n",
      "Summary step: 550\n",
      "Train loss: 0.424245\n",
      "Train accuracy:79.688%\n",
      "Test loss:0.448\n",
      "Test accuracy:78.365%\n",
      "Epoch: 2 Step: 100\n",
      "Summary step: 600\n",
      "Train loss: 0.467084\n",
      "Train accuracy:77.734%\n",
      "Test loss:0.436\n",
      "Test accuracy:79.233%\n",
      "Epoch: 2 Step: 150\n",
      "Summary step: 650\n",
      "Train loss: 0.436098\n",
      "Train accuracy:79.492%\n",
      "Test loss:0.437\n",
      "Test accuracy:79.235%\n",
      "Epoch: 2 Step: 200\n",
      "Summary step: 700\n",
      "Train loss: 0.414374\n",
      "Train accuracy:80.078%\n",
      "Test loss:0.444\n",
      "Test accuracy:78.763%\n",
      "Epoch: 3 Step: 0\n",
      "Summary step: 750\n",
      "Train loss: 0.446491\n",
      "Train accuracy:78.125%\n",
      "Test loss:0.446\n",
      "Test accuracy:78.549%\n",
      "Epoch: 3 Step: 50\n",
      "Summary step: 800\n",
      "Train loss: 0.41203\n",
      "Train accuracy:82.422%\n",
      "Test loss:0.436\n",
      "Test accuracy:79.314%\n",
      "Epoch: 3 Step: 100\n",
      "Summary step: 850\n",
      "Train loss: 0.438154\n",
      "Train accuracy:81.055%\n",
      "Test loss:0.426\n",
      "Test accuracy:79.804%\n",
      "Epoch: 3 Step: 150\n",
      "Summary step: 900\n",
      "Train loss: 0.425757\n",
      "Train accuracy:80.859%\n",
      "Test loss:0.434\n",
      "Test accuracy:79.708%\n",
      "Epoch: 3 Step: 200\n",
      "Summary step: 950\n",
      "Train loss: 0.402603\n",
      "Train accuracy:82.031%\n",
      "Test loss:0.426\n",
      "Test accuracy:80.090%\n",
      "Epoch: 4 Step: 0\n",
      "Summary step: 1000\n",
      "Train loss: 0.414904\n",
      "Train accuracy:79.102%\n",
      "Test loss:0.430\n",
      "Test accuracy:79.604%\n",
      "Epoch: 4 Step: 50\n",
      "Summary step: 1050\n",
      "Train loss: 0.392963\n",
      "Train accuracy:82.031%\n",
      "Test loss:0.425\n",
      "Test accuracy:79.961%\n",
      "Epoch: 4 Step: 100\n",
      "Summary step: 1100\n",
      "Train loss: 0.437851\n",
      "Train accuracy:80.078%\n",
      "Test loss:0.419\n",
      "Test accuracy:80.345%\n",
      "Epoch: 4 Step: 150\n",
      "Summary step: 1150\n",
      "Train loss: 0.405119\n",
      "Train accuracy:80.859%\n",
      "Test loss:0.422\n",
      "Test accuracy:80.255%\n",
      "Epoch: 4 Step: 200\n",
      "Summary step: 1200\n",
      "Train loss: 0.38781\n",
      "Train accuracy:81.641%\n",
      "Test loss:0.416\n",
      "Test accuracy:80.749%\n",
      "Epoch: 5 Step: 0\n",
      "Summary step: 1250\n",
      "Train loss: 0.40712\n",
      "Train accuracy:79.883%\n",
      "Test loss:0.421\n",
      "Test accuracy:80.243%\n",
      "Epoch: 5 Step: 50\n",
      "Summary step: 1300\n",
      "Train loss: 0.395593\n",
      "Train accuracy:82.031%\n",
      "Test loss:0.416\n",
      "Test accuracy:80.515%\n",
      "Epoch: 5 Step: 100\n",
      "Summary step: 1350\n",
      "Train loss: 0.414744\n",
      "Train accuracy:82.031%\n",
      "Test loss:0.411\n",
      "Test accuracy:80.687%\n",
      "Epoch: 5 Step: 150\n",
      "Summary step: 1400\n",
      "Train loss: 0.401064\n",
      "Train accuracy:82.227%\n",
      "Test loss:0.414\n",
      "Test accuracy:80.711%\n",
      "Epoch: 5 Step: 200\n",
      "Summary step: 1450\n",
      "Train loss: 0.378354\n",
      "Train accuracy:81.641%\n",
      "Test loss:0.411\n",
      "Test accuracy:80.946%\n",
      "Epoch: 6 Step: 0\n",
      "Summary step: 1500\n",
      "Train loss: 0.401037\n",
      "Train accuracy:81.250%\n",
      "Test loss:0.416\n",
      "Test accuracy:80.379%\n",
      "Epoch: 6 Step: 50\n",
      "Summary step: 1550\n",
      "Train loss: 0.381798\n",
      "Train accuracy:82.422%\n",
      "Test loss:0.410\n",
      "Test accuracy:80.866%\n",
      "Epoch: 6 Step: 100\n",
      "Summary step: 1600\n",
      "Train loss: 0.405668\n",
      "Train accuracy:81.641%\n",
      "Test loss:0.407\n",
      "Test accuracy:81.040%\n",
      "Epoch: 6 Step: 150\n",
      "Summary step: 1650\n",
      "Train loss: 0.402333\n",
      "Train accuracy:81.445%\n",
      "Test loss:0.409\n",
      "Test accuracy:81.093%\n",
      "Epoch: 6 Step: 200\n",
      "Summary step: 1700\n",
      "Train loss: 0.361333\n",
      "Train accuracy:83.203%\n",
      "Test loss:0.414\n",
      "Test accuracy:80.699%\n",
      "Epoch: 7 Step: 0\n",
      "Summary step: 1750\n",
      "Train loss: 0.411241\n",
      "Train accuracy:81.250%\n",
      "Test loss:0.420\n",
      "Test accuracy:80.303%\n",
      "Epoch: 7 Step: 50\n",
      "Summary step: 1800\n",
      "Train loss: 0.382247\n",
      "Train accuracy:82.617%\n",
      "Test loss:0.407\n",
      "Test accuracy:81.035%\n",
      "Epoch: 7 Step: 100\n",
      "Summary step: 1850\n",
      "Train loss: 0.410588\n",
      "Train accuracy:81.445%\n",
      "Test loss:0.406\n",
      "Test accuracy:81.230%\n",
      "Epoch: 7 Step: 150\n",
      "Summary step: 1900\n",
      "Train loss: 0.370108\n",
      "Train accuracy:83.008%\n",
      "Test loss:0.409\n",
      "Test accuracy:81.162%\n",
      "Epoch: 7 Step: 200\n",
      "Summary step: 1950\n",
      "Train loss: 0.355255\n",
      "Train accuracy:84.570%\n",
      "Test loss:0.410\n",
      "Test accuracy:80.966%\n",
      "Epoch: 8 Step: 0\n",
      "Summary step: 2000\n",
      "Train loss: 0.377722\n",
      "Train accuracy:82.422%\n",
      "Test loss:0.415\n",
      "Test accuracy:80.507%\n",
      "Epoch: 8 Step: 50\n",
      "Summary step: 2050\n",
      "Train loss: 0.358708\n",
      "Train accuracy:83.984%\n",
      "Test loss:0.402\n",
      "Test accuracy:81.359%\n",
      "Epoch: 8 Step: 100\n",
      "Summary step: 2100\n",
      "Train loss: 0.393633\n",
      "Train accuracy:82.812%\n",
      "Test loss:0.404\n",
      "Test accuracy:81.243%\n",
      "Epoch: 8 Step: 150\n",
      "Summary step: 2150\n",
      "Train loss: 0.352394\n",
      "Train accuracy:83.398%\n",
      "Test loss:0.405\n",
      "Test accuracy:81.382%\n",
      "Epoch: 8 Step: 200\n",
      "Summary step: 2200\n",
      "Train loss: 0.322714\n",
      "Train accuracy:86.133%\n",
      "Test loss:0.404\n",
      "Test accuracy:81.337%\n",
      "Epoch: 9 Step: 0\n",
      "Summary step: 2250\n",
      "Train loss: 0.393695\n",
      "Train accuracy:79.883%\n",
      "Test loss:0.413\n",
      "Test accuracy:80.584%\n",
      "Epoch: 9 Step: 50\n",
      "Summary step: 2300\n",
      "Train loss: 0.361473\n",
      "Train accuracy:83.398%\n",
      "Test loss:0.402\n",
      "Test accuracy:81.380%\n",
      "Epoch: 9 Step: 100\n",
      "Summary step: 2350\n",
      "Train loss: 0.38484\n",
      "Train accuracy:82.227%\n",
      "Test loss:0.399\n",
      "Test accuracy:81.712%\n",
      "Epoch: 9 Step: 150\n",
      "Summary step: 2400\n",
      "Train loss: 0.367079\n",
      "Train accuracy:84.766%\n",
      "Test loss:0.401\n",
      "Test accuracy:81.649%\n",
      "Epoch: 9 Step: 200\n",
      "Summary step: 2450\n",
      "Train loss: 0.330836\n",
      "Train accuracy:84.766%\n",
      "Test loss:0.404\n",
      "Test accuracy:81.495%\n",
      "Epoch: 10 Step: 0\n",
      "Summary step: 2500\n",
      "Train loss: 0.357936\n",
      "Train accuracy:84.375%\n",
      "Test loss:0.406\n",
      "Test accuracy:81.285%\n",
      "Epoch: 10 Step: 50\n",
      "Summary step: 2550\n",
      "Train loss: 0.33552\n",
      "Train accuracy:85.938%\n",
      "Test loss:0.401\n",
      "Test accuracy:81.445%\n",
      "Epoch: 10 Step: 100\n",
      "Summary step: 2600\n",
      "Train loss: 0.37944\n",
      "Train accuracy:83.008%\n",
      "Test loss:0.404\n",
      "Test accuracy:81.290%\n",
      "Epoch: 10 Step: 150\n",
      "Summary step: 2650\n",
      "Train loss: 0.344041\n",
      "Train accuracy:84.375%\n",
      "Test loss:0.402\n",
      "Test accuracy:81.569%\n",
      "Epoch: 10 Step: 200\n",
      "Summary step: 2700\n",
      "Train loss: 0.330918\n",
      "Train accuracy:84.766%\n",
      "Test loss:0.407\n",
      "Test accuracy:81.542%\n",
      "Epoch: 11 Step: 0\n",
      "Summary step: 2750\n",
      "Train loss: 0.356814\n",
      "Train accuracy:83.984%\n",
      "Test loss:0.418\n",
      "Test accuracy:80.707%\n",
      "Epoch: 11 Step: 50\n",
      "Summary step: 2800\n",
      "Train loss: 0.331633\n",
      "Train accuracy:84.961%\n",
      "Test loss:0.404\n",
      "Test accuracy:81.803%\n",
      "Epoch: 11 Step: 100\n",
      "Summary step: 2850\n",
      "Train loss: 0.374946\n",
      "Train accuracy:81.250%\n",
      "Test loss:0.405\n",
      "Test accuracy:81.237%\n",
      "Epoch: 11 Step: 150\n",
      "Summary step: 2900\n",
      "Train loss: 0.343238\n",
      "Train accuracy:84.180%\n",
      "Test loss:0.403\n",
      "Test accuracy:81.641%\n",
      "Epoch: 11 Step: 200\n",
      "Summary step: 2950\n",
      "Train loss: 0.320999\n",
      "Train accuracy:85.742%\n",
      "Test loss:0.412\n",
      "Test accuracy:81.626%\n",
      "Epoch: 12 Step: 0\n",
      "Summary step: 3000\n",
      "Train loss: 0.339372\n",
      "Train accuracy:85.742%\n",
      "Test loss:0.411\n",
      "Test accuracy:81.135%\n",
      "Epoch: 12 Step: 50\n",
      "Summary step: 3050\n",
      "Train loss: 0.338163\n",
      "Train accuracy:85.156%\n",
      "Test loss:0.414\n",
      "Test accuracy:81.601%\n",
      "Epoch: 12 Step: 100\n",
      "Summary step: 3100\n",
      "Train loss: 0.359815\n",
      "Train accuracy:83.789%\n",
      "Test loss:0.407\n",
      "Test accuracy:81.131%\n",
      "Epoch: 12 Step: 150\n",
      "Summary step: 3150\n",
      "Train loss: 0.316487\n",
      "Train accuracy:85.547%\n",
      "Test loss:0.411\n",
      "Test accuracy:81.379%\n",
      "Epoch: 12 Step: 200\n",
      "Summary step: 3200\n",
      "Train loss: 0.291684\n",
      "Train accuracy:87.891%\n",
      "Test loss:0.414\n",
      "Test accuracy:81.520%\n",
      "Epoch: 13 Step: 0\n",
      "Summary step: 3250\n",
      "Train loss: 0.341416\n",
      "Train accuracy:83.984%\n",
      "Test loss:0.414\n",
      "Test accuracy:81.245%\n",
      "Epoch: 13 Step: 50\n",
      "Summary step: 3300\n",
      "Train loss: 0.324882\n",
      "Train accuracy:84.570%\n",
      "Test loss:0.414\n",
      "Test accuracy:81.908%\n",
      "Epoch: 13 Step: 100\n",
      "Summary step: 3350\n",
      "Train loss: 0.329807\n",
      "Train accuracy:86.523%\n",
      "Test loss:0.411\n",
      "Test accuracy:80.978%\n",
      "Epoch: 13 Step: 150\n",
      "Summary step: 3400\n",
      "Train loss: 0.316963\n",
      "Train accuracy:86.328%\n",
      "Test loss:0.415\n",
      "Test accuracy:81.400%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 Step: 200\n",
      "Summary step: 3450\n",
      "Train loss: 0.293495\n",
      "Train accuracy:87.109%\n",
      "Test loss:0.415\n",
      "Test accuracy:81.632%\n",
      "Epoch: 14 Step: 0\n",
      "Summary step: 3500\n",
      "Train loss: 0.323399\n",
      "Train accuracy:85.352%\n",
      "Test loss:0.423\n",
      "Test accuracy:80.781%\n",
      "Epoch: 14 Step: 50\n",
      "Summary step: 3550\n",
      "Train loss: 0.302794\n",
      "Train accuracy:86.719%\n",
      "Test loss:0.421\n",
      "Test accuracy:81.529%\n",
      "Epoch: 14 Step: 100\n",
      "Summary step: 3600\n",
      "Train loss: 0.360516\n",
      "Train accuracy:83.594%\n",
      "Test loss:0.417\n",
      "Test accuracy:80.919%\n",
      "Epoch: 14 Step: 150\n",
      "Summary step: 3650\n",
      "Train loss: 0.341331\n",
      "Train accuracy:84.570%\n",
      "Test loss:0.425\n",
      "Test accuracy:81.160%\n",
      "Epoch: 14 Step: 200\n",
      "Summary step: 3700\n",
      "Train loss: 0.293079\n",
      "Train accuracy:86.523%\n",
      "Test loss:0.437\n",
      "Test accuracy:80.681%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'checkpoints/pretrained_bin_lstm.ckpt-3750'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import datetime\n",
    "import os\n",
    "import random\n",
    "# tf.flags.DEFINE_integer('train_steps', 1000,\n",
    "#                         'Number of training steps')\n",
    "\n",
    "# set variables\n",
    "num_epochs = 15\n",
    "tweet_size = 20\n",
    "hidden_size = 500\n",
    "vec_size = 300\n",
    "batch_size = 512\n",
    "number_of_layers= 2\n",
    "number_of_classes= 1\n",
    "starter_learning_rate = 0.001\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Create a session \n",
    "session = tf.Session()\n",
    "\n",
    "# Inputs placeholders\n",
    "tweets = tf.placeholder(tf.float32, [None, tweet_size, vec_size], \"tweets\")\n",
    "labels = tf.placeholder(tf.float32, [None], \"labels\")\n",
    "\n",
    "# Placeholder for dropout\n",
    "keep_prob = tf.placeholder_with_default(1.0,[], name=\"keep_prob\") \n",
    "\n",
    "# make the lstm cells, and wrap them in MultiRNNCell for multiple layers\n",
    "def lstm_cell():\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(hidden_size)\n",
    "    return tf.contrib.rnn.DropoutWrapper(cell=cell, input_keep_prob=keep_prob, output_keep_prob=keep_prob)\n",
    "\n",
    "multi_lstm_cells = tf.contrib.rnn.MultiRNNCell([lstm_cell() for _ in range(number_of_layers)], state_is_tuple=True)\n",
    "\n",
    "batch_size_T  = tf.shape(tweets)[0]\n",
    "zerostate = multi_lstm_cells.zero_state(batch_size_T, dtype=tf.float32)\n",
    "\n",
    "# Creates a recurrent neural network\n",
    "_, final_state = tf.nn.dynamic_rnn(multi_lstm_cells, tweets, dtype=tf.float32, initial_state=zerostate)\n",
    "\n",
    "sentiments = tf.contrib.layers.fully_connected(final_state[-1][-1], num_outputs=number_of_classes, activation_fn=None, weights_initializer=tf.random_normal_initializer(),\n",
    "  biases_initializer=tf.random_normal_initializer(), scope=\"fully_connected\")\n",
    "\n",
    "sentiments = tf.squeeze(sentiments, [1])\n",
    "\n",
    "predictions = tf.nn.sigmoid(sentiments, name=\"pred_op\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    # define cross entropy loss function\n",
    "    losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=sentiments, labels=labels)\n",
    "    loss = tf.reduce_mean(losses, name=\"loss_op\")\n",
    "    #tensorboard summaries\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    # round our actual probabilities to compute error\n",
    "    accuracy = tf.to_float(tf.equal(tf.to_float(tf.greater_equal(predictions, 0.5)), labels))\n",
    "    accuracy = tf.reduce_mean(tf.cast(accuracy, dtype=tf.float32), name=\"acc_op\")\n",
    "    #tensorboard summaries\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "    \n",
    "    \n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                           1000, 0.96, staircase=True)    \n",
    "# define our optimizer to minimize the loss\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train. AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "#tensorboard summaries\n",
    "merged_summary = tf.summary.merge_all()\n",
    "logdir = \"tensorboard/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "train_writer = tf.summary.FileWriter(logdir + \"-training\", session.graph)\n",
    "test_writer  = tf.summary.FileWriter(logdir + \"-testing\", session.graph)\n",
    "\n",
    "# initialize any variables\n",
    "tf.global_variables_initializer().run(session=session)\n",
    "\n",
    "# Create a saver for writing training checkpoints.\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# load our data and separate it into tweets and labels\n",
    "train_tweets = np.load('data_es/train_vec_tweets.npy')\n",
    "train_labels = np.load('data_es/train_vec_labels.npy')\n",
    "\n",
    "test_tweets = np.load('data_es/test_vec_tweets.npy')\n",
    "test_labels = np.load('data_es/test_vec_labels.npy')\n",
    "\n",
    "\n",
    "\n",
    "steps = int(len(train_tweets)/batch_size)\n",
    "summary_step = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for step in range(steps):\n",
    "\n",
    "        offset = (step * batch_size) % (len(train_tweets) - batch_size)\n",
    "        batch_tweets = train_tweets[offset : (offset + batch_size)]\n",
    "        batch_labels = train_labels[offset : (offset + batch_size)]\n",
    "\n",
    "        data = {tweets: batch_tweets, labels: batch_labels, keep_prob: 0.8}\n",
    "\n",
    "        #run operations in graph\n",
    "        _, loss_train, accuracy_train = session.run([optimizer, loss, accuracy], feed_dict=data)\n",
    "    \n",
    "        if (step % 50 == 0):\n",
    "            test_loss = []\n",
    "            test_accuracy = []\n",
    "            print(\"Epoch:\", epoch, \"Step:\", step)\n",
    "            print(\"Summary step:\",(summary_step))\n",
    "            print(\"Train loss:\", loss_train)\n",
    "            print(\"Train accuracy:%.3f%%\" % (accuracy_train*100))\n",
    "            \n",
    "            #tensorboard visualizations\n",
    "            summary = session.run(merged_summary, feed_dict=data)\n",
    "            train_writer.add_summary(summary, summary_step)\n",
    "            \n",
    "            for batch_num in range(int(len(test_tweets)/batch_size)):\n",
    "                \n",
    "                test_offset = (batch_num * batch_size) % (len(test_tweets) - batch_size)\n",
    "                test_batch_tweets = test_tweets[test_offset : (test_offset + batch_size)]\n",
    "                test_batch_labels = test_labels[test_offset : (test_offset + batch_size)]\n",
    "\n",
    "                data_testing = {tweets: test_batch_tweets, labels: test_batch_labels, keep_prob: 1.0}\n",
    "\n",
    "                loss_test, accuracy_test = session.run([loss, accuracy], feed_dict=data_testing)\n",
    "\n",
    "                test_loss.append(loss_test)\n",
    "                test_accuracy.append(accuracy_test)\n",
    "            \n",
    "            \n",
    "            test_offset = (random.randint(0,len(test_tweets) - batch_size) * batch_size) % (len(test_tweets) - batch_size)\n",
    "            test_batch_tweets = test_tweets[test_offset : (test_offset + batch_size)]\n",
    "            test_batch_labels = test_labels[test_offset : (test_offset + batch_size)]\n",
    "            \n",
    "            data_testing = {tweets: test_batch_tweets, labels: test_batch_labels, keep_prob: 1.0}\n",
    "\n",
    "            #tensorboard visualizations\n",
    "            test_summary = session.run(merged_summary, feed_dict=data_testing)\n",
    "            test_writer.add_summary(test_summary, summary_step)\n",
    "            \n",
    "            summary_step += 50\n",
    "            global_step += 50\n",
    "            \n",
    "            print(\"Test loss:%.3f\" % np.mean(test_loss))\n",
    "            print(\"Test accuracy:%.3f%%\" % (np.mean(test_accuracy)*100))\n",
    "\n",
    "\n",
    "saver.save(session, 'checkpoints/pretrained_bin_lstm.ckpt', global_step=summary_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "import numpy as np\n",
    "import random, re\n",
    "MAX_NB_WORDS=20\n",
    "\n",
    "def tweetsToVec(raw_tweets):\n",
    "\texclude = '!\"$%&\\'()*+,-./:;<=>?¿[\\\\]^_`{|}~'\n",
    "\tregex = re.compile('[%s]' % re.escape(exclude))\n",
    "\n",
    "\ttweets = []\n",
    "\n",
    "\ttknzr = TweetTokenizer()\n",
    "\tstop_words = set(stopwords.words('spanish'))\n",
    "\n",
    "\t#Cleaning tweets\n",
    "\tfor tweet in raw_tweets:\n",
    "\t\t#remove url\n",
    "\t\tno_url = re.sub(r\"https?\\S+\", \"\", tweet)\n",
    "\t\t#no punctuation\n",
    "\t\tno_pun = regex.sub('', no_url)\n",
    "\t\t#tokenize\n",
    "\t\ttokenized = tknzr.tokenize(no_pun)\n",
    "\t\t#remove stop words\n",
    "\t\timportant_words=[]\n",
    "\t\tfor word in tokenized:\n",
    "\t\t\tif not word[0] == '#':\n",
    "\t\t\t\timportant_words.append(word)\n",
    "\n",
    "\t\t# remove words if more than limit\n",
    "\n",
    "\t\ttweets.append(important_words)\n",
    "\n",
    "\t# load http://crscardellino.me/SBWCE/ trained model\n",
    "\tmodel = gensim.models.KeyedVectors.load_word2vec_format('data_process/SBW-vectors-300-min5.bin', binary=True)\n",
    "\n",
    "\tshape = (len(tweets), MAX_NB_WORDS, 300)\n",
    "\ttweets_tensor = np.zeros(shape, dtype=np.float32)\n",
    "\n",
    "\tfor i in range(len(tweets)):\n",
    "\t\t#vectorizing each word in the tweet with a vector shape = (300,)\n",
    "\t\tlength = len(tweets[i])\n",
    "\t\tfor f in range(length):\n",
    "\t\t\tword = tweets[i][f]\n",
    "\t\t\tif f >= MAX_NB_WORDS:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\t#if is not in the vocabulary\n",
    "\t\t\tif word in model.wv.vocab:\n",
    "\t\t\t\ttweets_tensor[i][f] = model.wv[word]\n",
    "\t\t\telse:\n",
    "\t\t\t\t#if it is a mention vectorize a name, for example @michael123 -> would be Carlos\n",
    "\t\t\t\tif word[0] == '@':\n",
    "\t\t\t\t\ttweets_tensor[i][f] = model.wv[name()]\n",
    "\t\t\t\t#if not append the unknown token\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\ttweets_tensor[i][f] = model.wv['unk']\n",
    "\t\t#End of sentence token\n",
    "\t\tif length - 1 < MAX_NB_WORDS:\n",
    "\t\t\ttweets_tensor[i][length - 1] = model.wv['eos']\n",
    "\n",
    "\tif len(tweets) == 1:\n",
    "\t\treturn tweets_tensor\n",
    "\telse:\n",
    "\t\treturn tweets_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints/pretrained_bin_lstm.ckpt-3750\n",
      "Prediction: [array([ 0.99933904,  0.10356666,  0.59075803,  0.02792856,  0.05727113,\n",
      "        0.93869275], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "sess = tf.Session()    \n",
    "#First let's load meta graph and restore weights\n",
    "saver = tf.train.import_meta_graph('checkpoints/pretrained_bin_lstm.ckpt-3750.meta')\n",
    "saver.restore(sess, tf.train.latest_checkpoint('./checkpoints'))\n",
    "\n",
    "# print(sess.run('saved:0'))\n",
    "# This will print 2, which is the value of bias that we saved\n",
    "\n",
    "graph = tf.get_default_graph()\n",
    "    \n",
    "# Access saved Variables directly\n",
    "tweets = graph.get_tensor_by_name(\"tweets:0\")\n",
    "# labels = graph.get_tensor_by_name(\"labels:0\")\n",
    "# keep_prob = graph.get_tensor_by_name(\"keep_prob:0\")\n",
    "\n",
    "# [print(tensor.name) for tensor in tf.get_default_graph().as_graph_def().node]\n",
    "\n",
    "# #Now, access the op that you want to run. \n",
    "pred = graph.get_tensor_by_name(\"pred_op:0\")\n",
    "# loss = graph.get_tensor_by_name(\"loss/loss_op:0\")\n",
    "# acc  = graph.get_tensor_by_name(\"accuracy/acc_op:0\")\n",
    "\n",
    "\n",
    "data = {tweets: tweetsToVec([\"Gran día para pasear en bici por Ka ciudad!\",\"Esto se complica...\",\"Me encanta la tarde\",\"Se me acabaron las vacaciones\",\"la puta madrea tengo un examen\",\"Muy bueno el cumple de Feli\"])}\n",
    "\n",
    "pred_ = sess.run([pred], feed_dict=data)\n",
    "\n",
    "print(\"Prediction:\", pred_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints/pretrained_bin_lstm.ckpt-3750\n",
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: b'models-binary/saved_model.pb'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'models-binary/saved_model.pb'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "export_dir = \"models-binary/\"\n",
    "\n",
    "sess=tf.Session()    \n",
    "#First let's load meta graph and restore weights\n",
    "saver = tf.train.import_meta_graph('checkpoints/pretrained_bin_lstm.ckpt-3750.meta')\n",
    "saver.restore(sess, tf.train.latest_checkpoint('./checkpoints'))\n",
    "\n",
    "\n",
    "graph = tf.get_default_graph()\n",
    "\n",
    "tweets = graph.get_tensor_by_name(\"tweets:0\")\n",
    "prediction = graph.get_tensor_by_name(\"pred_op:0\")\n",
    "\n",
    "builder = tf.saved_model.builder.SavedModelBuilder(export_dir)\n",
    "\n",
    "inputs  = tf.saved_model.utils.build_tensor_info(tweets)\n",
    "outputs = tf.saved_model.utils.build_tensor_info(prediction)\n",
    "\n",
    "legacy_init_op = tf.group(tf.tables_initializer(), name='legacy_init_op')\n",
    "\n",
    "prediction_signature = (\n",
    "tf.saved_model.signature_def_utils.build_signature_def(\n",
    "    inputs={'tweets': inputs},\n",
    "    outputs={'scores': outputs},\n",
    "    method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME))\n",
    "\n",
    "builder.add_meta_graph_and_variables(sess,[tf.saved_model.tag_constants.SERVING],\n",
    "    signature_def_map={\n",
    "        tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: prediction_signature\n",
    "    },legacy_init_op=legacy_init_op)\n",
    "\n",
    "\n",
    "builder.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gensim\n",
    "\n",
    "tf.reset_default_graph()\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('data_process/SBW-vectors-300-min5.bin', binary=True)\n",
    "\n",
    "embedding_matrix = np.zeros((len(model.wv.vocab), 300))\n",
    "for i in range(len(model.wv.vocab)):\n",
    "    embedding_vector = model.wv[model.wv.index2word[i]]\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "# model = gensim.models.KeyedVectors.load_word2vec_format('data_process/SBW-vectors-300-min5.bin', binary=True)\n",
    "# word = tf.Variable(\"hola\")\n",
    "# look = tf.nn.embedding_lookup(model.wv.index2word, 2)\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     res = sess.run(look)\n",
    "#     print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Prediction: [array([[  9.99706328e-01,   2.93696095e-04],\n",
    "       [  1.04312964e-01,   8.95687044e-01],\n",
    "       [  5.05031824e-01,   4.94968146e-01]], dtype=float32)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
